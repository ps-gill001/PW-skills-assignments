{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb7ed7ba-f20c-498b-a82e-e4706cafe66f",
   "metadata": {},
   "source": [
    "\n",
    "**Q1. What is Lasso Regression, and how does it differ from other regression techniques?**\n",
    "\n",
    "- **Lasso Regression**: Lasso stands for Least Absolute Shrinkage and Selection Operator. It is a regression technique that adds a regularization term to the linear regression equation, specifically a penalty term equal to the absolute value of the coefficients multiplied by a tuning parameter (λ).\n",
    "\n",
    "- **Differences**:\n",
    "  - **Ridge vs. Lasso**: Ridge Regression uses a penalty term proportional to the square of the coefficients (L2 norm), while Lasso uses a penalty term proportional to the absolute value of the coefficients (L1 norm).\n",
    "  - **Feature Selection**: Unlike Ridge, Lasso Regression can shrink coefficients to zero, effectively performing feature selection by removing less important variables from the model.\n",
    "\n",
    "**Q2. What is the main advantage of using Lasso Regression in feature selection?**\n",
    "\n",
    "- **Advantage**: Lasso Regression's main advantage in feature selection is its ability to shrink coefficients to exactly zero. This property allows for automatic feature selection, making the model simpler and potentially more interpretable by focusing only on the most relevant predictors.\n",
    "\n",
    "**Q3. How do you interpret the coefficients of a Lasso Regression model?**\n",
    "\n",
    "- **Interpretation**: The coefficients in Lasso Regression represent the relationship between each independent variable and the dependent variable, adjusted for the penalty term (λ * ||β||₁). Coefficients with non-zero values indicate predictors that are retained in the model and have a significant impact on the dependent variable.\n",
    "\n",
    "**Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?**\n",
    "\n",
    "- **Tuning Parameters**:\n",
    "  - **λ (Lambda)**: The regularization parameter controls the strength of the penalty term in Lasso Regression. Increasing λ increases the regularization strength, leading to more coefficients being shrunk towards zero.\n",
    "  - **α (Alpha)**: Lasso Regression can be generalized to include both L1 (Lasso) and L2 (Ridge) penalties through the elastic net regularization, where α controls the mix between L1 and L2 penalties.\n",
    "\n",
    "- **Effect on Performance**: \n",
    "  - **Higher λ**: Increases bias and reduces variance, leading to simpler models with potentially better generalization on unseen data.\n",
    "  - **Lower λ**: Reduces bias but may increase variance, potentially leading to overfitting if not properly tuned.\n",
    "\n",
    "**Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?**\n",
    "\n",
    "- **Non-linear Problems**: Lasso Regression is inherently a linear model and assumes a linear relationship between predictors and the response variable. For non-linear problems, transformations of predictors or using polynomial features can sometimes be applied to make the problem more amenable to linear regression techniques like Lasso.\n",
    "\n",
    "**Q6. What is the difference between Ridge Regression and Lasso Regression?**\n",
    "\n",
    "- **Differences**: \n",
    "  - **Penalty Type**: Ridge Regression uses an L2 penalty (sum of squares of coefficients), while Lasso Regression uses an L1 penalty (sum of absolute values of coefficients).\n",
    "  - **Feature Selection**: Ridge Regression can shrink coefficients towards zero but does not usually zero them out entirely, whereas Lasso Regression can zero out coefficients, effectively performing feature selection.\n",
    "\n",
    "**Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?**\n",
    "\n",
    "- **Multicollinearity**: Lasso Regression can handle multicollinearity by shrinking the coefficients of correlated variables towards zero. This helps in reducing the impact of multicollinearity on model stability and improves its interpretability by focusing on the most relevant predictors.\n",
    "\n",
    "**Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?**\n",
    "\n",
    "- **Choosing λ**: The optimal λ in Lasso Regression is typically chosen through cross-validation techniques such as k-fold cross-validation:\n",
    "  - **Grid Search**: Evaluate performance metrics (e.g., mean squared error) across a range of λ values.\n",
    "  - **Cross-Validation**: Select λ that minimizes prediction error on a validation set while avoiding overfitting on the training set.\n",
    "  - **Information Criterion**: Use criteria like AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) for model selection.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
