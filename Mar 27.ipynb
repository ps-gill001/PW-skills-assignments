{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2702857b-4eb5-4362-94a8-c498d8d03edd",
   "metadata": {},
   "source": [
    "\n",
    "**Q1. R-squared in Linear Regression:**\n",
    "\n",
    "- **Concept**: R-squared (R²) is a statistical measure that represents the proportion of the variance in the dependent variable (Y) that is explained by the independent variables (X) in a linear regression model.\n",
    "  \n",
    "- **Calculation**: R-squared is calculated as the ratio of the explained variance to the total variance of the dependent variable.\n",
    "  \n",
    "  \\[\n",
    "  R^2 = 1 - \\frac{\\text{Sum of Squared Residuals}}{\\text{Total Sum of Squares}}\n",
    "  \\]\n",
    "  \n",
    "  Where:\n",
    "  - Sum of Squared Residuals (SSR) measures the difference between predicted and actual values.\n",
    "  - Total Sum of Squares (SST) measures the variability of the dependent variable.\n",
    "\n",
    "- **Interpretation**: R-squared ranges from 0 to 1. Higher values indicate that more variance in the dependent variable is explained by the independent variables, suggesting a better fit of the model to the data.\n",
    "\n",
    "**Q2. Adjusted R-squared:**\n",
    "\n",
    "- **Definition**: Adjusted R-squared adjusts the R-squared value to penalize for adding irrelevant predictors that do not improve the model significantly.\n",
    "  \n",
    "- **Calculation**: Adjusted R-squared is calculated using the formula:\n",
    "  \n",
    "  \\[\n",
    "  \\text{Adjusted } R^2 = 1 - \\left( \\frac{(1 - R^2) \\cdot (n - 1)}{n - p - 1} \\right)\n",
    "  \\]\n",
    "  \n",
    "  Where:\n",
    "  - \\( n \\) is the number of observations.\n",
    "  - \\( p \\) is the number of predictors.\n",
    "\n",
    "- **Difference**: Adjusted R-squared tends to be lower than R-squared when irrelevant predictors are added, making it a more conservative measure of model fit.\n",
    "\n",
    "**Q3. When to Use Adjusted R-squared:**\n",
    "\n",
    "- **Appropriateness**: Adjusted R-squared is more appropriate when comparing models with different numbers of predictors or when evaluating the inclusion of additional predictors in the model.\n",
    "\n",
    "**Q4. RMSE, MSE, and MAE:**\n",
    "\n",
    "- **RMSE (Root Mean Squared Error)**: Measures the average deviation of predicted values from actual values, emphasizing larger errors due to the square root.\n",
    "\n",
    "  \\[\n",
    "  \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "  \\]\n",
    "\n",
    "- **MSE (Mean Squared Error)**: Similar to RMSE but without the square root, penalizing larger errors more significantly.\n",
    "\n",
    "  \\[\n",
    "  \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "  \\]\n",
    "\n",
    "- **MAE (Mean Absolute Error)**: Measures the average absolute difference between predicted and actual values, providing a linear representation of error.\n",
    "\n",
    "  \\[\n",
    "  \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "  \\]\n",
    "\n",
    "- **Interpretation**: Lower values of RMSE, MSE, and MAE indicate better model performance in terms of prediction accuracy.\n",
    "\n",
    "**Q5. Advantages and Disadvantages of RMSE, MSE, and MAE:**\n",
    "\n",
    "- **Advantages**:\n",
    "  - **RMSE and MSE**: Emphasize larger errors, useful when large errors are critical.\n",
    "  - **MAE**: Robust to outliers, provides a direct interpretation of average error magnitude.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - **RMSE and MSE**: Sensitive to outliers due to squaring of errors.\n",
    "  - **MAE**: Does not differentiate between large and small errors as effectively as RMSE.\n",
    "\n",
    "**Q6. Lasso Regularization:**\n",
    "\n",
    "- **Concept**: Lasso (Least Absolute Shrinkage and Selection Operator) regularization adds a penalty term to the linear regression cost function, encouraging simpler models by shrinking less important coefficients to zero.\n",
    "\n",
    "- **Difference from Ridge**: Lasso tends to perform variable selection by eliminating coefficients of less important predictors, whereas Ridge only shrinks coefficients towards zero without eliminating them entirely.\n",
    "\n",
    "- **Appropriateness**: Lasso is more appropriate when there is a need for feature selection or when dealing with high-dimensional data with potentially redundant predictors.\n",
    "\n",
    "**Q7. Preventing Overfitting with Regularized Linear Models:**\n",
    "\n",
    "- **Example**: In a regression model predicting housing prices, applying Lasso regularization can shrink coefficients of less influential predictors (like neighborhood features) towards zero, preventing them from overly influencing predictions based on noisy data.\n",
    "\n",
    "**Q8. Limitations of Regularized Linear Models:**\n",
    "\n",
    "- **Constraints**: Regularization methods may oversimplify models, potentially discarding useful predictors.\n",
    "- **Data Sensitivity**: Performance heavily depends on the quality and relevance of predictors selected for regularization.\n",
    "- **Complexity**: Choosing an optimal regularization parameter (e.g., λ in Ridge and α in Lasso) requires careful tuning to balance bias and variance effectively.\n",
    "\n",
    "**Q9. Choosing Between Models Based on Evaluation Metrics:**\n",
    "\n",
    "- **Decision**: RMSE of 10 for Model A and MAE of 8 for Model B suggests Model B has lower average prediction error, making it potentially better. However, the choice also depends on specific project goals, data characteristics, and interpretability requirements.\n",
    "\n",
    "- **Limitations**: Choice of metric (RMSE vs. MAE) may overlook other important aspects like interpretability, computational efficiency, or domain-specific requirements.\n",
    "\n",
    "**Q10. Comparing Regularized Linear Models:**\n",
    "\n",
    "- **Decision**: Choosing between Ridge (λ = 0.1) and Lasso (α = 0.5) depends on the trade-off between variable selection (Lasso's advantage) and continuous shrinkage (Ridge's advantage). \n",
    "\n",
    "- **Trade-offs**: Ridge may be preferable for scenarios where all predictors are potentially relevant but need regularization, while Lasso is more suitable for feature selection or when predictors may be redundant.\n",
    "\n",
    "These explanations should provide a comprehensive understanding of the concepts related to linear regression, regularization, evaluation metrics, and their practical applications. If you have more questions or need further clarification, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f90e6e-ef89-4a65-bc63-416381fbcaff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
