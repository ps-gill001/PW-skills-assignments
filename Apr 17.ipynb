{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9757b902-40cd-4c5b-bd3f-22b6f2c438c4",
   "metadata": {},
   "source": [
    "\n",
    "### Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Gradient Boosting Regression is a machine learning technique that builds an ensemble of weak learners (typically decision trees) sequentially. It aims to minimize a loss function (such as mean squared error for regression problems) by adding models to the ensemble, where each new model corrects errors made by the previous ones. \n",
    "\n",
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "Here's a simplified implementation of Gradient Boosting Regression using Python and NumPy. We'll use a simple dataset and evaluate the model's performance:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "        self.losses = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize with the mean of y\n",
    "        initial_prediction = np.mean(y)\n",
    "        self.estimators.append(lambda X: initial_prediction)\n",
    "        self.losses.append(np.mean((y - initial_prediction) ** 2))\n",
    "\n",
    "        # Iteratively fit weak learners\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - self.predict(X)\n",
    "            tree = self._fit_tree(X, residuals)\n",
    "            self.estimators.append(tree)\n",
    "            predictions = tree.predict(X)\n",
    "            self.losses.append(np.mean((y - np.sum([self.learning_rate * tree.predict(X) for tree in self.estimators], axis=0)) ** 2))\n",
    "\n",
    "    def _fit_tree(self, X, residuals):\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "        tree.fit(X, residuals)\n",
    "        return tree\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.sum([self.learning_rate * tree.predict(X) for tree in self.estimators], axis=0)\n",
    "        return predictions\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        mse = np.mean((y - predictions) ** 2)\n",
    "        r2 = 1 - np.sum((y - predictions) ** 2) / np.sum((y - np.mean(y)) ** 2)\n",
    "        return mse, r2\n",
    "\n",
    "# Example usage:\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X[:, 0] + np.random.normal(scale=2, size=100)\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "model.fit(X, y)\n",
    "mse, r2 = model.evaluate(X, y)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "```\n",
    "\n",
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimize the performance of the model. Use grid search or random search to find the best hyperparameters.\n",
    "\n",
    "Here's an example of how you might perform hyperparameter tuning using GridSearchCV from scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Create the model and GridSearchCV instance\n",
    "gb_model = GradientBoostingRegressor()\n",
    "grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "mse, r2 = best_model.evaluate(X, y)\n",
    "print(f\"Mean Squared Error (best model): {mse}\")\n",
    "print(f\"R-squared (best model): {r2}\")\n",
    "```\n",
    "\n",
    "### Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "A weak learner in Gradient Boosting is a model that is slightly better than random guessing for the problem at hand. In the case of regression, weak learners are typically shallow decision trees (with limited depth) that predict continuous values. Each weak learner contributes a small improvement to the overall prediction accuracy of the ensemble.\n",
    "\n",
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "The intuition behind Gradient Boosting is to sequentially add models to an ensemble, where each new model corrects errors made by the previous ones. By focusing on instances where the model's predictions are incorrect (using gradients or residuals), Gradient Boosting iteratively builds a strong learner from many weak learners.\n",
    "\n",
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Gradient Boosting builds an ensemble of weak learners by iteratively training new models to correct the errors (residuals) made by the previous models. Each new weak learner is trained on a modified version of the data where the weights of the instances are adjusted to focus more on the previously misclassified instances.\n",
    "\n",
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "1. **Initialize with a simple model**: Start with an initial prediction, often the mean of the target variable.\n",
    "2. **Compute residuals**: Calculate the residuals (errors) between the predicted values and the actual target values.\n",
    "3. **Fit a weak learner**: Train a weak learner (e.g., decision tree) to predict the residuals.\n",
    "4. **Update predictions**: Update the ensemble's predictions by adding the weak learner's prediction scaled by a learning rate.\n",
    "5. **Iterate**: Repeat steps 2-4, each time fitting a new weak learner to predict the residuals left by the previous ensemble.\n",
    "\n",
    "This iterative process minimizes a loss function (e.g., mean squared error) and results in a strong learner that combines the predictions of multiple weak learners.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b98f846-4c3d-4360-82db-075dc36eb493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
