{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cebb306-a851-4f28-9909-522ba7273c42",
   "metadata": {},
   "source": [
    "\n",
    "### Q1. What is the KNN algorithm?\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a non-parametric and lazy learning algorithm used for both classification and regression tasks. It works on the principle of finding the K nearest data points in the feature space to a given query point and then using the majority class (for classification) or the average value (for regression) of these neighbors to make predictions.\n",
    "\n",
    "### Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "Choosing the value of K is crucial in KNN as it directly affects the model's performance:\n",
    "- **Small K**: More flexible model, prone to noise and overfitting.\n",
    "- **Large K**: Smoother decision boundaries, but may lead to underfitting.\n",
    "\n",
    "Typically, K is chosen through techniques like:\n",
    "- **Cross-validation**: Evaluate performance for different K values.\n",
    "- **Grid search**: Systematically search through a range of K values to find the optimal one based on a chosen metric (e.g., accuracy, RMSE).\n",
    "\n",
    "### Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "- **KNN Classifier**: Predicts the class label of a new data point based on the majority class of its K nearest neighbors.\n",
    "- **KNN Regressor**: Predicts the continuous value (numeric) of a new data point by averaging the values of its K nearest neighbors.\n",
    "\n",
    "### Q4. How do you measure the performance of KNN?\n",
    "\n",
    "For classification tasks, common performance metrics include:\n",
    "- **Accuracy**: Proportion of correctly classified instances.\n",
    "- **Precision**: Proportion of true positive predictions among all positive predictions.\n",
    "- **Recall**: Proportion of true positive predictions among all actual positive instances.\n",
    "- **F1-score**: Harmonic mean of precision and recall.\n",
    "\n",
    "For regression tasks, common metrics include:\n",
    "- **Mean Squared Error (MSE)**: Average squared difference between predicted and actual values.\n",
    "- **Root Mean Squared Error (RMSE)**: Square root of MSE, which is in the same units as the target variable.\n",
    "- **R-squared (Coefficient of Determination)**: Proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "\n",
    "### Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "The curse of dimensionality refers to the issue where the feature space becomes increasingly sparse as the number of dimensions (features) grows. In KNN, this can lead to:\n",
    "- Increased computational complexity.\n",
    "- Difficulty in defining a meaningful distance metric.\n",
    "- Increased risk of overfitting due to the sparsity of data points in high-dimensional spaces.\n",
    "\n",
    "### Q6. How do you handle missing values in KNN?\n",
    "\n",
    "Handling missing values in KNN can be approached by methods such as:\n",
    "- **Imputation**: Replace missing values with a sensible estimate (e.g., mean, median, mode).\n",
    "- **KNN-based imputation**: Use the K nearest neighbors to impute missing values based on similar instances.\n",
    "\n",
    "### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "- **KNN Classifier**: Suitable for classification problems where the decision boundaries may be irregular and not easily defined by simple linear models.\n",
    "- **KNN Regressor**: Suitable for regression problems where the relationship between predictors and response is not linear and may have complex interactions.\n",
    "\n",
    "The choice between classifier and regressor depends on the nature of the problem (classification vs. regression) and the underlying data characteristics (continuous vs. categorical outcomes).\n",
    "\n",
    "### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "- **Strengths**:\n",
    "  - Simple and intuitive.\n",
    "  - No training phase (lazy learning).\n",
    "  - Non-parametric nature handles complex decision boundaries.\n",
    "\n",
    "- **Weaknesses**:\n",
    "  - Computationally expensive during prediction, especially with large datasets.\n",
    "  - Sensitive to irrelevant features and outliers.\n",
    "  - Requires careful preprocessing (scaling, handling missing data).\n",
    "\n",
    "Address weaknesses by:\n",
    "- Optimize K through cross-validation.\n",
    "- Preprocess data to handle outliers and scale features appropriately.\n",
    "- Reduce dimensionality if possible to mitigate computational cost and curse of dimensionality.\n",
    "\n",
    "### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "- **Euclidean Distance**: Measures the straight-line distance between two points in Euclidean space. For two points \\( (x_1, y_1) \\) and \\( (x_2, y_2) \\), Euclidean distance is \\( \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} \\).\n",
    "\n",
    "- **Manhattan Distance**: Measures the sum of the absolute differences between the coordinates of two points. For two points \\( (x_1, y_1) \\) and \\( (x_2, y_2) \\), Manhattan distance is \\( |x_2 - x_1| + |y_2 - y_1| \\).\n",
    "\n",
    "### Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "Feature scaling is crucial in KNN because it ensures that all features contribute equally to the distance computations between data points. Since KNN uses distance metrics (like Euclidean or Manhattan distance), features with larger scales or variances can dominate the distance calculation. Common scaling techniques include standardization (subtracting the mean and dividing by the standard deviation) or normalization (scaling features to a fixed range).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a306e-6b4c-4e97-b3c0-e80886b54bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
