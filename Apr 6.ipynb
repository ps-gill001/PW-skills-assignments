{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25d4773-e9d9-4031-b3d4-ed9d6a96225a",
   "metadata": {},
   "source": [
    "**Q1. What is the mathematical formula for a linear SVM?**\n",
    "\n",
    "In a linear Support Vector Machine (SVM) for binary classification, the decision boundary is defined by a hyperplane. The mathematical formulation of the linear SVM can be represented as:\n",
    "\n",
    "\\[ f(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b \\]\n",
    "\n",
    "where:\n",
    "- \\( \\mathbf{x} \\) represents the input features.\n",
    "- \\( \\mathbf{w} \\) is the weight vector.\n",
    "- \\( b \\) is the bias term.\n",
    "\n",
    "The decision function \\( f(\\mathbf{x}) \\) determines on which side of the hyperplane a data point lies. Specifically:\n",
    "- \\( f(\\mathbf{x}) > 0 \\) indicates one class.\n",
    "- \\( f(\\mathbf{x}) < 0 \\) indicates the other class.\n",
    "\n",
    "**Q2. What is the objective function of a linear SVM?**\n",
    "\n",
    "The objective function of a linear SVM aims to maximize the margin (distance between the hyperplane and the nearest data points of either class) while minimizing the classification error. Mathematically, the objective function for a linear SVM can be formulated as:\n",
    "\n",
    "\\[ \\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\]\n",
    "\n",
    "subject to:\n",
    "\\[ y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\quad \\text{for all } i \\]\n",
    "\n",
    "where:\n",
    "- \\( \\mathbf{x}_i \\) are the training examples.\n",
    "- \\( y_i \\) are the class labels (\\( y_i = \\pm 1 \\)).\n",
    "\n",
    "The constraints ensure that each data point is correctly classified with a margin of at least 1.\n",
    "\n",
    "**Q3. What is the kernel trick in SVM?**\n",
    "\n",
    "The kernel trick allows SVMs to efficiently perform nonlinear classification by implicitly mapping the input space into a higher-dimensional feature space where a linear separation can be achieved. This is done by replacing the dot product \\( \\mathbf{x}_i \\cdot \\mathbf{x}_j \\) in the SVM objective function with a kernel function \\( K(\\mathbf{x}_i, \\mathbf{x}_j) \\):\n",
    "\n",
    "\\[ \\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\]\n",
    "\n",
    "subject to:\n",
    "\\[ y_i(\\sum_{j=1}^{n} \\alpha_j y_j K(\\mathbf{x}_i, \\mathbf{x}_j) + b) \\geq 1 \\quad \\text{for all } i \\]\n",
    "\n",
    "Common kernels include linear, polynomial, Gaussian (RBF), and sigmoid kernels.\n",
    "\n",
    "**Q4. What is the role of support vectors in SVM? Explain with an example.**\n",
    "\n",
    "Support vectors are the data points that lie closest to the decision boundary (hyperplane) and influence the position and orientation of the hyperplane. These points are crucial because they define the margin and, hence, the optimal separating hyperplane.\n",
    "\n",
    "Example:\n",
    "- Consider a binary classification problem with two classes, where the classes are not linearly separable in the input feature space.\n",
    "- SVM finds a hyperplane that maximizes the margin between the two classes.\n",
    "- Support vectors are the data points from both classes that are closest to the hyperplane or are misclassified.\n",
    "\n",
    "**Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM?**\n",
    "\n",
    "- **Hyperplane**: A hyperplane in SVM separates data points of different classes. In a 2D space, it's a straight line, and in higher dimensions, it's a plane.\n",
    "  \n",
    "- **Margin**: The margin is the distance between the hyperplane and the nearest data points of each class. \n",
    "\n",
    "- **Soft Margin and Hard Margin**:\n",
    "  - **Hard Margin**: Requires all data points to be correctly classified with no margin violations. Only feasible when data is perfectly separable.\n",
    "  - **Soft Margin**: Allows for some misclassifications and margin violations to achieve a wider margin and a more generalized model.\n",
    "\n",
    "Here's a graphical representation:\n",
    "- **Hard Margin SVM**:\n",
    "  \n",
    "  ![Hard Margin SVM](https://i.imgur.com/lhhtv12.png)\n",
    "  \n",
    "  In this example, the hard margin SVM tries to find a hyperplane that separates the classes perfectly without allowing any margin violations.\n",
    "\n",
    "- **Soft Margin SVM**:\n",
    "  \n",
    "  ![Soft Margin SVM](https://i.imgur.com/04ghZfU.png)\n",
    "  \n",
    "  Here, the soft margin SVM allows for some margin violations (points inside the margin) to accommodate the misclassified points and achieve a wider margin.\n",
    "\n",
    "**Q6. SVM Implementation through Iris dataset.**\n",
    "\n",
    "Let's implement a linear SVM classifier on the Iris dataset using Python:\n",
    "\n",
    "```python\n",
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Taking only the first two features for simplicity\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "svm_clf = SVC(kernel='linear', C=1.0)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Plot the decision boundary\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.title('Linear SVM Decision Boundary')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This code performs the following:\n",
    "- Loads the Iris dataset.\n",
    "- Splits it into training and testing sets.\n",
    "- Trains a linear SVM classifier (`SVC` with `kernel='linear'`).\n",
    "- Predicts labels on the test set and calculates accuracy.\n",
    "- Plots the decision boundary using the first two features (`sepal length` vs `sepal width`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd20359-992a-4974-bd60-00458fea6228",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
