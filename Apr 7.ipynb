{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada39f80-cdf3-4860-8ea2-dabdd0820b78",
   "metadata": {},
   "source": [
    "\n",
    "**Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?**\n",
    "\n",
    "In machine learning algorithms, kernel functions are used to transform input data into higher-dimensional feature spaces where non-linear relationships can be captured by linear models. Polynomial functions can be used as kernel functions, which allow SVMs (Support Vector Machines) to model non-linear decision boundaries effectively.\n",
    "\n",
    "The relationship:\n",
    "- **Polynomial Kernel Function**: The polynomial kernel function \\( K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i^T \\mathbf{x}_j + r)^d \\) computes the dot product of the feature vectors \\( \\mathbf{x}_i \\) and \\( \\mathbf{x}_j \\) in a higher-dimensional space defined by the degree \\( d \\), coefficient \\( \\gamma \\), and bias term \\( r \\).\n",
    "- **Kernel Trick**: Instead of explicitly computing the transformation to the higher-dimensional space, the kernel function computes the dot product directly, which is computationally efficient.\n",
    "\n",
    "**Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?**\n",
    "\n",
    "Here's how you can implement an SVM with a polynomial kernel using Scikit-learn:\n",
    "\n",
    "```python\n",
    "# Importing necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of SVM with polynomial kernel\n",
    "svm_poly = SVC(kernel='poly', degree=3, gamma='scale', C=1.0)\n",
    "svm_poly.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = svm_poly.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- `SVC(kernel='poly')` specifies the SVM with a polynomial kernel.\n",
    "- `degree=3` specifies the degree of the polynomial kernel.\n",
    "- `gamma='scale'` uses a default scaling factor for gamma.\n",
    "- `C=1.0` is the regularization parameter.\n",
    "\n",
    "Adjust the `degree`, `gamma`, and `C` parameters based on your dataset and problem requirements.\n",
    "\n",
    "**Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?**\n",
    "\n",
    "In Support Vector Regression (SVR), epsilon (Îµ) defines the margin of tolerance where no penalty is given to errors. Increasing epsilon can lead to a wider margin, allowing more data points to be within the margin of tolerance. Consequently, this can increase the number of support vectors because more data points may fall within or near the margin region.\n",
    "\n",
    "**Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?**\n",
    "\n",
    "- **Kernel Function**: Different kernel functions (e.g., linear, polynomial, Gaussian RBF) capture different types of relationships in the data. For non-linear relationships, polynomial or RBF kernels are often more effective.\n",
    "- **C Parameter**: Controls the trade-off between achieving a low training error and a smooth decision function. Higher values of C allow for more complex decision boundaries, potentially leading to overfitting. Lower values of C increase the margin width, leading to a simpler model.\n",
    "- **Epsilon Parameter**: Defines the margin of tolerance where no penalty is given to errors. Larger values of epsilon allow for more errors to be within the margin of tolerance.\n",
    "- **Gamma Parameter**: Influences the reach of the individual training samples, where low values imply far reach and high values imply close reach. Higher gamma values lead to more complex decision boundaries, potentially overfitting the training data.\n",
    "\n",
    "Examples:\n",
    "- **Increase C**: When you suspect the data might have noise or you need a more complex decision boundary to capture the intricacies of the problem.\n",
    "- **Increase Gamma**: When the training data has complex relationships or you want the model to focus more on closer points.\n",
    "- **Increase Degree (for Polynomial Kernel)**: When the relationships in the data appear to be polynomial in nature and you want to capture higher-order interactions.\n",
    "\n",
    "**Q5. Assignment: Implementing SVM Classifier in Python**\n",
    "\n",
    "Here's a structured approach to implementing an SVM classifier on a dataset, tuning hyperparameters, and saving the trained model:\n",
    "\n",
    "```python\n",
    "# Importing necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of SVM classifier\n",
    "svm = SVC(kernel='rbf', gamma='scale', C=1.0)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = svm.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier (accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Tune hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10, 100],\n",
    "              'gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n",
    "              'kernel': ['linear', 'poly', 'rbf']}\n",
    "grid_search = GridSearchCV(estimator=SVC(), param_grid=param_grid, cv=5, verbose=1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Cross-validation Score: {best_score:.2f}\")\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "best_svm = grid_search.best_estimator_\n",
    "best_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(best_svm, 'svm_classifier.pkl')\n",
    "print(\"Trained classifier saved to svm_classifier.pkl\")\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- We load the Iris dataset and split it into training and testing sets.\n",
    "- We preprocess the data by scaling it using `StandardScaler`.\n",
    "- We create an instance of the SVM classifier (`SVC`) with an RBF kernel and default hyperparameters.\n",
    "- We train the classifier on the scaled training data and evaluate its accuracy on the test set.\n",
    "- We then use `GridSearchCV` to tune the hyperparameters (`C`, `gamma`, `kernel`) using cross-validation (`cv=5`).\n",
    "- We print the best parameters and the best cross-validation score obtained.\n",
    "- Finally, we train the tuned SVM classifier on the entire training set and save it to a file (`svm_classifier.pkl`) for future use.\n",
    "\n",
    "This example demonstrates a complete workflow from loading data to model training, hyperparameter tuning, evaluation, and model persistence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade72b4-85fe-4482-9ed6-4de8da463b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
